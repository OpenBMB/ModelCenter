<div align="center">

<h1>ğŸ› ModelCenter</h1>

**é«˜æ•ˆä½èµ„æºçš„å¤§æ¨¡å‹å®ç°**

</div>

<p align="center">
  <a href="#æ€»è§ˆ">æ€»è§ˆ</a> â€¢
  <a href="#æ–‡æ¡£">æ–‡æ¡£</a> â€¢
  <a href="#å®‰è£…">å®‰è£…</a> â€¢
  <a href="#å¿«é€Ÿä¸Šæ‰‹">å¿«é€Ÿä¸Šæ‰‹</a> â€¢
  <a href="#æ¨¡å‹æ”¯æŒ">æ¨¡å‹æ”¯æŒ</a> â€¢
  <a href="./README.md" target="_blank">English</a>
</p>

<p align="center">

<a href='https://modelcenter.readthedocs.io/en/latest/?badge=latest'>
    <img src='https://readthedocs.org/projects/modelcenter/badge/?version=latest' alt='Documentation Status' />
</a>

<a href="https://github.com/OpenBMB/ModelCenter/releases">
    <img alt="GitHub release (latest by date including pre-releases)" src="https://img.shields.io/github/v/release/OpenBMB/ModelCenter?include_prereleases">
</a>

<a href="https://github.com/OpenBMB/ModelCenter/blob/main/LICENSE">
    <img alt="GitHub" src="https://img.shields.io/github/license/OpenBMB/ModelCenter">
</a>

</p>

## æœ€æ–°åŠ¨æ€

- 2022/03/16 [0.1.0](https://github.com/OpenBMB/ModelCenter/releases/tag/v0.0.1-beta) ModelCenter å…¬å¼€å‘å¸ƒäº†ç¬¬ä¸€ä¸ªç¨³å®šç‰ˆæœ¬, ä¿®å¤äº†ä¸€äº›æ¨¡å‹æ€§èƒ½ä¸Šå’Œæ˜¾å­˜å ç”¨ä¸Šçš„é—®é¢˜.
- 2022/03/21 [0.0.1-beta](https://github.com/OpenBMB/ModelCenter/releases/tag/v0.0.1-beta) ModelCenter å…¬å¼€å‘å¸ƒäº†ç¬¬ä¸€ä¸ª beta ç‰ˆæœ¬.

## æ€»è§ˆ

ModelCenter åŸºäº [OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain/) å®ç°äº†ä¸€ç³»åˆ—ç»å…¸çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚ ModelCenter åœ¨æ¨¡å‹å®ç°ä¸Šçš„å®—æ—¨æ˜¯ é«˜æ•ˆã€ä½èµ„æºä¸é«˜å¯ç”¨æ€§, å¹¶ä¸”èƒ½å¤Ÿæ”¯æŒåˆ†å¸ƒå¼çš„è®­ç»ƒ.

æˆ‘ä»¬çš„ä¸»è¦ä¼˜åŠ¿æœ‰ï¼š

- æ˜“ç”¨æ€§ï¼šç›¸æ¯” Deepspeed, Megatron, æˆ‘ä»¬æ‹¥æœ‰æ›´å¥½æ›´çµæ´»çš„å°è£…ï¼Œä¸”é…ç½® python ç¯å¢ƒå®¹æ˜“, è®­ç»ƒä»£ç ä¸ pytorch é£æ ¼ç»Ÿä¸€ã€‚
- æ›´é«˜æ•ˆçš„æ˜¾å­˜åˆ©ç”¨ï¼šæ¨¡å‹å ç”¨æ˜¾å­˜è¾ƒå¤§æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ GPU çš„è®¡ç®—èƒ½åŠ›æœªè¢«å……åˆ†ä½¿ç”¨æ—¶æ˜¾å­˜å ç”¨å°±å·²ç»è·‘æ»¡ã€‚æˆ‘ä»¬çš„å®ç°å¯ä»¥å°†æ˜¾å­˜å ç”¨é™ä½æ•°å€ï¼Œè¿›è€Œä½¿ç”¨æ›´å¤§çš„ batch-size å¯¹ GPU çš„è®¡ç®—èƒ½åŠ›è¿›è¡Œæ›´å……åˆ†çš„åˆ©ç”¨ã€‚
- ä½èµ„æºçš„é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒï¼šåœ¨ [OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain/) çš„æ”¯æŒä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°† ZeRO3 çš„ä¼˜åŒ–è½»æ˜“åœ°æ‰©å±•è‡³å„å¤§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„é€šä¿¡å’Œè°ƒåº¦ä¸Šä½œå‡ºä¼˜åŒ–ã€‚

## æ–‡æ¡£

æˆ‘ä»¬çš„[æ–‡æ¡£](https://modelcenter.readthedocs.io/)æä¾›äº†å…³äºå·¥å…·åŒ…çš„æ›´å¤šä¿¡æ¯ã€‚

## å®‰è£…

### 1. ç”¨ pip å®‰è£… (æ¨è)

```shell
$ pip install model-center
```

### 2. ä»æºä»£ç å®‰è£…

```shell
$ git clone https://github.com/OpenBMB/ModelCenter.git
$ cd ModelCenter
$ pip install -r requirements.txt
$ python3 setup.py install
```

## å¿«é€Ÿä¸Šæ‰‹

## æ¨¡å‹æ”¯æŒ

- [CPM: A Large-scale Generative Chinese Pre-trained Language Model.](https://arxiv.org/abs/2012.00413) Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``CPM1.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - cpm1-large

- [CPM-2: Large-scale Cost-efficient Pre-trained Language Models.](https://arxiv.org/abs/2106.10715) Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``CPM2.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - cpm2-large

- [Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding.](https://arxiv.org/abs/1810.04805) Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``Bert.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - bert-base-cased
    - bert-base-uncased
    - bert-large-cased
    - bert-large-uncased
    - bert-base-chinese
    - bert-base-multilingual-cased

- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``T5.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - t5-small
    - t5-base
    - t5-large
    - t5-3b
    - t5-11b

- [GPT2: Language Models are Unsupervised Multitask Learners.](http://www.persagen.com/files/misc/radford2019language.pdf) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``GPT2.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - gpt2-base
    - gpt2-medium
    - gpt2-large
    - gpt2-xl

- [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax) (from EleutherAI) released in the repo [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) by Ben Wang and Aran Komatsuzaki. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``GPTj.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - gptj-6b

## è¿è¡Œæ€§èƒ½

ä½ å¯ä»¥åœ¨ [BMTrain](https://github.com/OpenBMB/BMTrain) ä»“åº“ä¸­æ‰¾åˆ°æ›´å¤šçš„æ€§èƒ½æµ‹è¯•æ•ˆæœ.

## å¼€æºç¤¾åŒº

æ¬¢è¿è´¡çŒ®è€…å‚ç…§æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](https://github.com/OpenBMB/ModelCenter/blob/main/CONTRIBUTING.md)è´¡çŒ®ç›¸å…³ä»£ç ã€‚

æ‚¨ä¹Ÿå¯ä»¥åœ¨å…¶ä»–å¹³å°ä¸æˆ‘ä»¬æ²Ÿé€šäº¤æµ:
- QQç¾¤: 735930538
- å®˜æ–¹ç½‘ç«™: http://www.openbmb.org
- å¾®åš: http://weibo.cn/OpenBMB
- Twitter: https://twitter.com/OpenBMB

## å¼€æºè®¸å¯

è¯¥å·¥å…·åŒ…ä½¿ç”¨[Apache 2.0](https://github.com/OpenBMB/ModelCenter/blob/main/LICENSE)å¼€æºè®¸å¯è¯ã€‚
<div align="center">

<h1>ğŸ› ModelCenter</h1>

**é«˜æ•ˆä½èµ„æºçš„å¤§æ¨¡å‹å®ç°**

</div>

<p align="center">
  <a href="#æ€»è§ˆ">æ€»è§ˆ</a> â€¢
  <a href="#æ–‡æ¡£">æ–‡æ¡£</a> â€¢
  <a href="#å®‰è£…">å®‰è£…</a> â€¢
  <a href="#å¿«é€Ÿä¸Šæ‰‹">å¿«é€Ÿä¸Šæ‰‹</a> â€¢
  <a href="#æ¨¡å‹æ”¯æŒ">æ¨¡å‹æ”¯æŒ</a> â€¢
  <a href="./README.md" target="_blank">English</a>
</p>

<p align="center">

<a href='https://modelcenter.readthedocs.io/en/latest/?badge=latest'>
    <img src='https://readthedocs.org/projects/modelcenter/badge/?version=latest' alt='Documentation Status' />
</a>

<a href="https://github.com/OpenBMB/ModelCenter/releases">
    <img alt="GitHub release (latest by date including pre-releases)" src="https://img.shields.io/github/v/release/OpenBMB/ModelCenter?include_prereleases">
</a>

<a href="https://github.com/OpenBMB/ModelCenter/blob/main/LICENSE">
    <img alt="GitHub" src="https://img.shields.io/github/license/OpenBMB/ModelCenter">
</a>

</p>

## æœ€æ–°åŠ¨æ€

- 2022/03/16 [0.1.0](https://github.com/OpenBMB/ModelCenter/releases/tag/v0.0.1-beta) ModelCenter å…¬å¼€å‘å¸ƒäº†ç¬¬ä¸€ä¸ªç¨³å®šç‰ˆæœ¬, ä¿®å¤äº†ä¸€äº›æ¨¡å‹æ€§èƒ½ä¸Šå’Œæ˜¾å­˜å ç”¨ä¸Šçš„é—®é¢˜.
- 2022/03/21 [0.0.1-beta](https://github.com/OpenBMB/ModelCenter/releases/tag/v0.0.1-beta) ModelCenter å…¬å¼€å‘å¸ƒäº†ç¬¬ä¸€ä¸ª beta ç‰ˆæœ¬.

## æ€»è§ˆ

ModelCenter åŸºäº [OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain/) å®ç°äº†ä¸€ç³»åˆ—ç»å…¸çš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚ ModelCenter åœ¨æ¨¡å‹å®ç°ä¸Šçš„å®—æ—¨æ˜¯ é«˜æ•ˆã€ä½èµ„æºä¸é«˜å¯ç”¨æ€§, å¹¶ä¸”èƒ½å¤Ÿæ”¯æŒåˆ†å¸ƒå¼çš„è®­ç»ƒ.

æˆ‘ä»¬çš„ä¸»è¦ä¼˜åŠ¿æœ‰ï¼š

- æ˜“ç”¨æ€§ï¼šç›¸æ¯” Deepspeed, Megatron, æˆ‘ä»¬æ‹¥æœ‰æ›´å¥½æ›´çµæ´»çš„å°è£…ï¼Œä¸”é…ç½® python ç¯å¢ƒå®¹æ˜“, è®­ç»ƒä»£ç ä¸ pytorch é£æ ¼ç»Ÿä¸€ã€‚
- æ›´é«˜æ•ˆçš„æ˜¾å­˜åˆ©ç”¨ï¼šæ¨¡å‹å ç”¨æ˜¾å­˜è¾ƒå¤§æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ GPU çš„è®¡ç®—èƒ½åŠ›æœªè¢«å……åˆ†ä½¿ç”¨æ—¶æ˜¾å­˜å ç”¨å°±å·²ç»è·‘æ»¡ã€‚æˆ‘ä»¬çš„å®ç°å¯ä»¥å°†æ˜¾å­˜å ç”¨é™ä½æ•°å€ï¼Œè¿›è€Œä½¿ç”¨æ›´å¤§çš„ batch-size å¯¹ GPU çš„è®¡ç®—èƒ½åŠ›è¿›è¡Œæ›´å……åˆ†çš„åˆ©ç”¨ã€‚
- ä½èµ„æºçš„é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒï¼šåœ¨ [OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain/) çš„æ”¯æŒä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå°† ZeRO3 çš„ä¼˜åŒ–è½»æ˜“åœ°æ‰©å±•è‡³å„å¤§é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¹¶åœ¨åˆ†å¸ƒå¼è®­ç»ƒçš„é€šä¿¡å’Œè°ƒåº¦ä¸Šä½œå‡ºä¼˜åŒ–ã€‚

## æ–‡æ¡£

æˆ‘ä»¬çš„[æ–‡æ¡£](https://modelcenter.readthedocs.io/)æä¾›äº†å…³äºå·¥å…·åŒ…çš„æ›´å¤šä¿¡æ¯ã€‚

## å®‰è£…

### 1. ç”¨ pip å®‰è£… (æ¨è)

```shell
$ pip install model-center
```

### 2. ä»æºä»£ç å®‰è£…

```shell
$ git clone https://github.com/OpenBMB/ModelCenter.git
$ cd ModelCenter
$ pip install -r requirements.txt
$ python3 setup.py install
```

## å¿«é€Ÿä¸Šæ‰‹

åœ¨æœ¬èŠ‚ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•åœ¨ä¸€ä¸ªåˆ†ç±»æ•°æ®é›†ä¸Šå¾®è°ƒ[BERT](https://arxiv.org/abs/1810.04805)æ¨¡å‹ã€‚

### 1. åˆå§‹åŒ–BMTrainåç«¯
é¦–å…ˆï¼Œä½ éœ€è¦åœ¨ä»£ç å¼€å¤´å¼•å…¥`bmtrain`å¹¶ä½¿ç”¨`bmtrain.init_distributed()`ã€‚

```python
# init bmtrain backend
import bmtrain as bmt
bmt.init_distributed(seed=0)
```

### 2. å‡†å¤‡æ¨¡å‹
æ¥ä¸‹æ¥ï¼Œä½ å¯ä»¥ä»`model_center`ä¸­è·å–é¢„è®­ç»ƒå¥½çš„BERTæ¨¡å‹ï¼Œä¾‹å¦‚*bert-base-uncased*ã€‚ç”±äºæˆ‘ä»¬æ˜¯åœ¨ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒBERTæ¨¡å‹ï¼Œæ‰€ä»¥éœ€è¦åœ¨æœ€åä¸€å±‚åæ·»åŠ ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚

```python
import torch
from model_center.model import Bert, BertConfig
from model_center.layer import Linear

class BertModel(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = Bert.from_pretrained("bert-base-uncased")
        self.dense = Linear(config.dim_model, 2)
        bmt.init_parameters(self.dense)

    def forward(self, input_ids, attention_mask):
        pooler_output = self.bert(input_ids=input_ids, attention_mask=attention_mask).pooler_output
        logits = self.dense(pooler_output)
        return logits

config = BertConfig.from_pretrained("bert-base-uncased")
model = BertModel(config)
```

### 3. å‡†å¤‡æ•°æ®é›†
ä¸‹ä¸€æ­¥æ˜¯å‡†å¤‡æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒå’ŒéªŒè¯æ¨¡å‹ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨[SuperGLUE benchmark](https://super.gluebenchmark.com/)ä¸­çš„[BoolQ](https://github.com/google-research-datasets/boolean-questions)æ•°æ®é›†ã€‚ä½ éœ€è¦ä¸‹è½½è¯¥æ•°æ®é›†ï¼Œå¹¶å°†è§£å‹åçš„æ–‡ä»¶å¤¹æ”¾åœ¨`your_path_to_dataset`è·¯å¾„ä¸‹ã€‚

```python
from model_center.dataset.bertdataset import DATASET
from model_center.dataset import DistributedDataLoader
from model_center.tokenizer import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
splits = ['train', 'dev']
dataset = {}

for split in splits:
    dataset[split] = DATASET['BoolQ']('your_path_to_dataset', split, bmt.rank(), bmt.world_size(), tokenizer, max_encoder_length=512)

batch_size = 64
train_dataloader = DistributedDataLoader(dataset['train'], batch_size=batch_size, shuffle=True)
dev_dataloader = DistributedDataLoader(dataset['dev'], batch_size=batch_size, shuffle=False)
```

### 4. è®­ç»ƒæ¨¡å‹
ç°åœ¨ï¼Œåœ¨è®¾ç½®ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥å’ŒæŸå¤±å‡½æ•°åï¼Œå°±å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼è¿™é‡Œï¼Œæˆ‘ä»¬è®­ç»ƒBERTæ¨¡å‹5è½®ï¼Œå¹¶ä¸”åœ¨æ¯è½®è®­ç»ƒç»“æŸåéªŒè¯æ¨¡å‹çš„æ€§èƒ½ã€‚

```python
optimizer = bmt.optim.AdamOffloadOptimizer(model.parameters())

lr_scheduler = bmt.lr_scheduler.Noam(
    optimizer, 
    start_lr = 1e-5,
    warmup_iter = 100, 
    end_iter = -1)

loss_func = bmt.loss.FusedCrossEntropy(ignore_index=-100)

for epoch in range(5):
    model.train()
    for data in train_dataloader:
        input_ids = data['input_ids']
        attention_mask = data['attention_mask']
        labels = data['labels']

        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        logits = model(input_ids, attention_mask)

        # è®¡ç®—æŸå¤±
        loss = loss_func(logits.view(-1, logits.shape[-1]), labels.view(-1))

        # ç¼©æ”¾æŸå¤±ä»¥é¿å…fp16ç²¾åº¦ä¸‹æº¢
        loss = optimizer.loss_scale(loss)

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        grad_norm = bmt.optim.clip_grad_norm(optimizer.param_groups, max_norm=10.0, scale = optimizer.scale, norm_type = 2)

        bmt.optim_step(optimizer, lr_scheduler)

        # åœ¨åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œåªåœ¨rankä¸º0æ—¶æ‰“å°ä¿¡æ¯
        # ä½¿ç”¨bmt.sum_loss(loss)èšåˆæ‰€æœ‰è¿›ç¨‹ä¸Šçš„æŸå¤±
        bmt.print_rank(
            "loss: {:.4f} | lr: {:.4e}, scale: {:10.4f} | grad_norm: {:.4f} |".format(
                bmt.sum_loss(loss).item(),
                lr_scheduler.current_lr,
                int(optimizer.scale),
                grad_norm,
            )
        )

    # éªŒè¯æ¨¡å‹çš„æ€§èƒ½
    model.eval()
    with torch.no_grad():
        pd = [] # é¢„æµ‹ç»“æœ
        gt = [] # æ ‡ç­¾
        for data in dev_dataloader:
            input_ids = data["input_ids"]
            attention_mask = data["attention_mask"]
            labels = data["labels"]

            logits = model(input_ids, attention_mask)
            loss = loss_func(logits.view(-1, logits.shape[-1]), labels.view(-1))

            logits = logits.argmax(dim=-1)

            pd.extend(logits.cpu().tolist())
            gt.extend(labels.cpu().tolist())

        # èšåˆæ‰€æœ‰è¿›ç¨‹ä¸Šçš„ç»“æœ
        pd = bmt.gather_result(torch.tensor(pd).int()).cpu().tolist()
        gt = bmt.gather_result(torch.tensor(gt).int()).cpu().tolist()

        # è®¡ç®—æŒ‡æ ‡
        from sklearn.metrics import accuracy_score
        acc = accuracy_score(gt, pd)
        bmt.print_rank(f"accuracy: {acc*100:.2f}")
```

### 5. è¿è¡Œä»£ç 
ä½ å¯ä»¥ä½¿ç”¨PyTorchåŸç”Ÿçš„åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨å™¨æ¥è¿è¡Œä¸Šè¿°ä»£ç ï¼Œæ ¹æ®ä½ çš„PyTorchç‰ˆæœ¬é€‰æ‹©ä¸‹åˆ—å‘½ä»¤ä¸­çš„ä¸€ä¸ªã€‚

* `${MASTER_ADDR}` means the IP address of the master node.
* `${MASTER_PORT}` means the port of the master node.
* `${NNODES}` means the total number of nodes.
* `${GPU_PER_NODE}` means the number of GPUs per node.
* `${NODE_RANK}` means the rank of this node.

#### torch.distributed.launch
```shell
$ python3 -m torch.distributed.launch --master_addr ${MASTER_ADDR} --master_port ${MASTER_PORT} --nproc_per_node ${GPU_PER_NODE} --nnodes ${NNODES} --node_rank ${NODE_RANK} train.py
```

#### torchrun

```shell
$ torchrun --nnodes=${NNODES} --nproc_per_node=${GPU_PER_NODE} --rdzv_id=1 --rdzv_backend=c10d --rdzv_endpoint=${MASTER_ADDR}:${MASTER_PORT} train.py
```

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒPyTorch[å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/distributed.html#launch-utility)ã€‚


## æ¨¡å‹æ”¯æŒ

- [CPM: A Large-scale Generative Chinese Pre-trained Language Model.](https://arxiv.org/abs/2012.00413) Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``CPM1.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - cpm1-large

- [CPM-2: Large-scale Cost-efficient Pre-trained Language Models.](https://arxiv.org/abs/2106.10715) Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chaojun Xiao, Zhenbo Sun, Yuan Yao, Fanchao Qi, Jian Guan, Pei Ke, Yanzheng Cai, Guoyang Zeng, Zhixing Tan, Zhiyuan Liu, Minlie Huang, Wentao Han, Yang Liu, Xiaoyan Zhu, Maosong Sun. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``CPM2.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - cpm2-large

- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.](https://arxiv.org/abs/1810.04805) Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``Bert.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - bert-base-cased
    - bert-base-uncased
    - bert-large-cased
    - bert-large-uncased
    - bert-base-chinese
    - bert-base-multilingual-cased

- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``T5.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - t5-small
    - t5-base
    - t5-large
    - t5-3b
    - t5-11b

- [GPT2: Language Models are Unsupervised Multitask Learners.](http://www.persagen.com/files/misc/radford2019language.pdf) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``GPT2.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - gpt2-base
    - gpt2-medium
    - gpt2-large
    - gpt2-xl

- [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax) (from EleutherAI) released in the repo [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) by Ben Wang and Aran Komatsuzaki. æˆ‘ä»¬æ”¯æŒä½¿ç”¨ ``GPTj.from_pretrained(identifier)`` æ¥åŠ è½½ä¸‹åˆ—æ¨¡å‹ï¼š

    - gptj-6b

## è¿è¡Œæ€§èƒ½

ä½ å¯ä»¥åœ¨ [OpenBMB/BMTrain](https://github.com/OpenBMB/BMTrain) ä»“åº“ä¸­æ‰¾åˆ°æ›´å¤šçš„æ€§èƒ½æµ‹è¯•æ•ˆæœ.

## å¼€æºç¤¾åŒº

æ¬¢è¿è´¡çŒ®è€…å‚ç…§æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](https://github.com/OpenBMB/ModelCenter/blob/main/CONTRIBUTING.md)è´¡çŒ®ç›¸å…³ä»£ç ã€‚

æ‚¨ä¹Ÿå¯ä»¥åœ¨å…¶ä»–å¹³å°ä¸æˆ‘ä»¬æ²Ÿé€šäº¤æµ:
- QQç¾¤: 735930538
- å®˜æ–¹ç½‘ç«™: http://www.openbmb.org
- å¾®åš: http://weibo.cn/OpenBMB
- Twitter: https://twitter.com/OpenBMB

## å¼€æºè®¸å¯

è¯¥å·¥å…·åŒ…ä½¿ç”¨[Apache 2.0](https://github.com/OpenBMB/ModelCenter/blob/main/LICENSE)å¼€æºè®¸å¯è¯ã€‚